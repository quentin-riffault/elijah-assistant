{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Find the correct action according to text query (command)\n",
    "\n",
    "Our issue is to find an action that is coherent with a command that is given to the model. How to find a correct answer ? Let's take the example of Alexa. Alexa probably uses a neural-network model in order to get a coherent response. This can work with huge neural networks on cloud-driven architectures. However, on a simpler architecture, it may seem harder to do so. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Method 1 : Incremental database learning\n",
    "\n",
    "This method uses a command database in order to find the adequate answer. The user will give a command string to the model and the model will have to answer. If the answer is not what the user wants, then the user associates an action to his command and the database is updated.\n",
    "\n",
    "![](commands-doc/IDL-flow1.svg)\n",
    "\n",
    "### Query preprocessing\n",
    "\n",
    "The goal here is to clean the query from unnecessary words (such as articles)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['éteins', 'télévision']\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "def remove_punctuation_and_cap(query: str) -> str:\n",
    "    return query.replace(\",\", \"\").replace(\".\", \"\").lower()\n",
    "            \n",
    "def remove_stopwords(query: str, words: set) -> list:\n",
    "    processed_list = list()\n",
    "    \n",
    "    for word in query.split():\n",
    "        if word not in words:\n",
    "            processed_list.append(word)\n",
    "            \n",
    "    return processed_list\n",
    "\n",
    "def clean_query(query: str, words: set) -> list:\n",
    "    \n",
    "    start = time.time()\n",
    "\n",
    "    query = remove_punctuation_and_cap(query)\n",
    "            \n",
    "    return remove_stopwords(query, words)\n",
    "\n",
    "print(clean_query(\"le elijah, éteins la télévision le le\", [\"elijah\", \"la\", \"le\", \"cette\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to modelize the data ?\n",
    "\n",
    "### Naive representation\n",
    "\n",
    "The data could be modelized as a word list from a query and as a response string. These word lists would be hashed in order to be stored. The hash should have low-dispersion in order for queries to be close to each other when they share the same semantics\n",
    "\n",
    "![](commands-doc/data-storage.svg)\n",
    "\n",
    "### Vectorizing the words\n",
    "\n",
    "Another solution would be to vectorize the words before. With this, we could be able to classify words, which would be quite useful. Classes would represent the answer that the model would give to the user. Thanks to this classification, we could say \"This vector has xx% of chances to match the question\". Database selection could be done with the distance to the barycenter of the topics.\n",
    "\n",
    "![](commands-doc/data-storage2.svg)\n",
    "\n",
    "## Searching\n",
    "\n",
    "### Naive search\n",
    "\n",
    "The easiest version should be to compare query hashes. This is easy to implement but quite inefficient with highly-changing commands. As a consequence, using this search method is not advised as the user would have to put a lot of different commands' aliases when the model could guess what the user wants.\n",
    "\n",
    "### Vector-based search\n",
    "\n",
    "The challenge here is to give a value to each word in order to build a vector. This said vector is going to be transformed in a scalar value with a barycenter computation.\n",
    "\n",
    "#### Naive method\n",
    "\n",
    "We keep each word in memory and assign a random weight to them. When a vector is computed, we look in this table. Consistency is then kept, however, we can't be sure that it is going to be coherent and efficient, especially with new values..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Implementation\n",
    "\n",
    "Those values must be initialized for the words below (see below)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "\n",
    "barycenter_list = list()\n",
    "word_map = dict() # map < String, int >\n",
    "\n",
    "known_words = set([\"télévision\", \"allumer\", \"éteindre\"])\n",
    "\n",
    "max_range = 200\n",
    "\n",
    "for word in known_words:\n",
    "    word_map[word] = (random.randrange(-max_range, max_range))/max_range"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Computation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.18\n",
      "-0.805\n"
     ]
    }
   ],
   "source": [
    "def valuation(word : str) -> float:\n",
    "    if word in word_map:\n",
    "        return word_map[word]\n",
    "    word_map[word] = (random.randrange(-max_range, max_range))/max_range\n",
    "    return word_map[word]\n",
    "\n",
    "print(valuation(\"télévision\"))\n",
    "print(valuation(\"Poupipou\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each word in the sentence (that has been cleaned), a valuation is associated to it and put in the vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorize(sentence : list) -> np.array:\n",
    "    v = np.zeros((1, len(sentence)))\n",
    "    \n",
    "    for i, word in enumerate(sentence):\n",
    "        v[0][i] = valuation(word)\n",
    "    \n",
    "    return v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%script false --no-raise-error\n",
    "# ^ Prevents execution\n",
    "sentence = \"Elijah, allume la télévision\"\n",
    "stoplist = [\"elijah\", \"le\", \"la\"]\n",
    "words = clean_query(sentence, stoplist)\n",
    "\n",
    "\n",
    "v1 = vectorize(clean_query(\"Elijah, allume la télévision\", stoplist))\n",
    "v2 = vectorize(clean_query(\"Elijah, éteins la télévision\", stoplist))\n",
    "v3 = vectorize(clean_query(\"Elijah, allume la télé\", stoplist))\n",
    "v4 = vectorize(clean_query(\"Elijah, éteins la télé\", stoplist))\n",
    "\n",
    "print(np.linalg.norm(v1 - v2))\n",
    "print(np.linalg.norm(v1 - v3))\n",
    "print(np.linalg.norm(v1 - v4))\n",
    "print(np.linalg.norm(v2 - v3))\n",
    "print(np.linalg.norm(v2 - v4))\n",
    "print(np.linalg.norm(v3 - v4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using new rules\n",
    "\n",
    "As expected, since values are chosen randomly, similar words can't be linked together and give values that are very different. \n",
    "In order to link new words with older words, a distance needs to be computed.\n",
    "There are three types of word:\n",
    "- **New words**, that have nothing to do with the others\n",
    "    - These words need to have a random value\n",
    "- **Abreviated words**\n",
    "    - These words must have the same value as the word they represent.\n",
    "- **Alternative words**\n",
    "    - These words need not to change the barycenter of the class they belong.\n",
    "    - To simplify things, alternative words will follow abreviated words' rules.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "'(' was never closed (801916683.py, line 42)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"/tmp/ipykernel_23295/801916683.py\"\u001b[0;36m, line \u001b[0;32m42\u001b[0m\n\u001b[0;31m    print(valuation(\"écran\")\u001b[0m\n\u001b[0m         ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m '(' was never closed\n"
     ]
    }
   ],
   "source": [
    "def abreviate(word: str) -> str:\n",
    "    \n",
    "    for ws in set(word_map.keys()):\n",
    "        if word in ws: # if the word is a substring of a key\n",
    "            return ws\n",
    "    \n",
    "    return None\n",
    "\n",
    "def synonym(word: str) -> str: \n",
    "    \n",
    "    # Think of an API use (synonymes.net)\n",
    "    \n",
    "    return None\n",
    "\n",
    "def valuation(word : str) -> float:\n",
    "    if word in word_map: # Word exists\n",
    "        print(f\"Word {word} exists\")\n",
    "        return word_map[word]\n",
    "    \n",
    "    # New word\n",
    "    \n",
    "    abrev = abreviate(word)\n",
    "    syn  = synonym(word)\n",
    "    \n",
    "    if abrev: # The word is a synonym\n",
    "        print(f\"{word} is an abreviation of {abrev}\")\n",
    "        word_map[word] = word_map[abrev]\n",
    "        return word_map[abrev]\n",
    "        \n",
    "    elif syn: # The word is an abreviation\n",
    "        print(f\"Word {word} is an synonym of {syn}\")\n",
    "        word_map[word] = word_map[syn]\n",
    "        return word_map[syn]\n",
    "        \n",
    "    # the word is completely new\n",
    "    print(f\"Word {word} is completely new\")\n",
    "    word_map[word] = (random.randrange(-max_range, max_range))/max_range\n",
    "    return word_map[word]\n",
    "\n",
    "print(valuation(\"télévision\"))\n",
    "print(valuation(\"télé\"))\n",
    "print(valuation(\"écran\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = \"Elijah, allume la télévision\"\n",
    "stoplist = [\"elijah\", \"le\", \"la\"]\n",
    "words = clean_query(sentence, stoplist)\n",
    "\n",
    "\n",
    "v1 = vectorize(clean_query(\"Elijah, allume la télévision\", stoplist))\n",
    "v2 = vectorize(clean_query(\"Elijah, éteins la télévision\", stoplist))\n",
    "v3 = vectorize(clean_query(\"Elijah, allume la télé\", stoplist))\n",
    "v4 = vectorize(clean_query(\"Elijah, éteins la télé\", stoplist))\n",
    "\n",
    "print(np.linalg.norm(v1 - v2))\n",
    "print(np.linalg.norm(v1 - v3))\n",
    "print(np.linalg.norm(v1 - v4))\n",
    "print(np.linalg.norm(v2 - v3))\n",
    "print(np.linalg.norm(v2 - v4))\n",
    "print(np.linalg.norm(v3 - v4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "e7370f93d1d0cde622a1f8e1c04877d8463912d04d973331ad4851f04de6915a"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
